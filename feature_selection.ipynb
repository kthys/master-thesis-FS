{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2, SelectKBest\n",
    "from ITMO_FS import filters\n",
    "from numpy import inf\n",
    "from geneticAlg import GeneticSelection\n",
    "from itertools import permutations\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the columns for NSL-KDD\n",
    "\n",
    "columns = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "                    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "                    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "                    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "                    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "                    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "                    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "                    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "                    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "                    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"labels\"]\n",
    "\n",
    "\n",
    "nslkddTrain = pd.read_csv('./NSL-KDD/KDDTrain+_20Percent.txt', sep=\",\", header=None, usecols = [i for i in range(42)], names=columns)\n",
    "\n",
    "# Convert boolean features to objects to avoid misinterpretation in NSL-KDD\n",
    "nslkddTrain['land'] = nslkddTrain['land'].astype('object', copy=False)\n",
    "nslkddTrain['logged_in'] = nslkddTrain['logged_in'].astype('object', copy=False)\n",
    "nslkddTrain['urgent'] = nslkddTrain['urgent'].astype('object', copy=False)\n",
    "nslkddTrain['is_host_login'] = nslkddTrain['is_host_login'].astype('object', copy=False)\n",
    "nslkddTrain['is_guest_login'] = nslkddTrain['is_guest_login'].astype('object', copy=False)\n",
    "\n",
    "# Import UNSW-NB15 train set\n",
    "\n",
    "unswTrain = pd.DataFrame()\n",
    "unswTrain = pd.read_csv('./UNSW-NB15/UNSW_NB15_training-set.csv', sep=\",\", na_values=[' '])\n",
    "\n",
    "# Convert boolean features to objects to avoid misinterpretation in UNSW-NB15\n",
    "unswTrain['is_ftp_login'] = unswTrain['is_ftp_login'].astype('object', copy=False)\n",
    "unswTrain['is_sm_ips_ports'] = unswTrain['is_sm_ips_ports'].astype('object', copy=False)\n",
    "unswTrain['label'] = unswTrain['label'].astype('int32', copy=False)\n",
    "del unswTrain[\"attack_cat\"]\n",
    "del unswTrain[\"id\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes\n",
    "print(\"shape of NSL-KDD train set\", nslkddTrain.shape)\n",
    "print(\"shape of UNSW train set: \", unswTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the values removed by the authors of GA-LR\n",
    "removedCols = ['land', 'urgent', 'num_failed_logins', 'root_shell', 'su_attempted', 'num_shells', 'num_outbound_cmds', 'is_host_login']\n",
    "for col in removedCols:\n",
    "    print(col)\n",
    "    print(nslkddTrain[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Labelencode the target\n",
    "nslkddTrain['labels'] = nslkddTrain['labels'].map(lambda x: 0 if (x == 'normal') else 1)\n",
    "\n",
    "# Perform ordinal encoding on remaining string values\n",
    "toEncodeKdd = list(nslkddTrain.select_dtypes(include=['object']).columns)\n",
    "toEncodeUnsw = list(unswTrain.select_dtypes(include=['object']).columns)\n",
    "\n",
    "OrdinalEncoder.get_feature_names_out = (lambda self, names=None: self.feature_names_in_)\n",
    "\n",
    "cat_encoding = Pipeline([\n",
    "    ('cat_encoder', OrdinalEncoder())\n",
    "    ])\n",
    "\n",
    "ctEncodingKdd = ColumnTransformer([\n",
    "    ('cat', cat_encoding, toEncodeKdd)\n",
    "    ], remainder=\"passthrough\", verbose_feature_names_out=False)\n",
    "\n",
    "ctEncodingUnsw = ColumnTransformer([\n",
    "('cat', cat_encoding, toEncodeUnsw)\n",
    "], remainder=\"passthrough\", verbose_feature_names_out=False)\n",
    "\n",
    "nslkddTrainEncoded = pd.DataFrame.from_records(ctEncodingKdd.fit_transform(nslkddTrain), columns=ctEncodingKdd.get_feature_names_out())\n",
    "unswTrainEncoded = pd.DataFrame.from_records(ctEncodingUnsw.fit_transform(unswTrain), columns=ctEncodingUnsw.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log base 10 to columns containing large values\n",
    "largevalueskdd = ['duration', 'src_bytes', 'dst_bytes', 'num_compromised', 'num_root', 'count', 'srv_count', 'dst_host_count', 'dst_host_srv_count']\n",
    "largevaluesunsw = ['spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin', 'smean', 'dmean', 'trans_depth', 'response_body_len']\n",
    "\n",
    "for colname in largevalueskdd:\n",
    "    nslkddTrainEncoded[colname] = np.log10(nslkddTrainEncoded[colname])\n",
    "    nslkddTrainEncoded.replace([-np.inf], 0, inplace=True)\n",
    "\n",
    "for colname in largevaluesunsw:\n",
    "    unswTrainEncoded[colname] = np.log10(unswTrain[colname])\n",
    "    unswTrainEncoded[colname].replace([-np.inf], 0, inplace=True)\n",
    "\n",
    "# Apply minmax scaler on numerical values\n",
    "MinMaxScaler.get_feature_names_out = (lambda self, names=None: self.feature_names_in_)\n",
    "\n",
    "numColskdd = list(nslkddTrainEncoded.select_dtypes(include=['float64', 'int64']).columns)\n",
    "numColsunsw = list(unswTrainEncoded.select_dtypes(include=['float64', 'int64']).columns)\n",
    "\n",
    "num_minmaxscaling = Pipeline([\n",
    "    ('num_minmaxscaling', MinMaxScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "ctMinMaxkdd = ColumnTransformer([\n",
    "    ('minmax', num_minmaxscaling, numColskdd)\n",
    "], remainder=\"passthrough\", verbose_feature_names_out=False)\n",
    "\n",
    "ctMinMaxunsw = ColumnTransformer([\n",
    "    ('minmax', num_minmaxscaling, numColsunsw)\n",
    "], remainder=\"passthrough\", verbose_feature_names_out=False)\n",
    "\n",
    "\n",
    "nslkddTrainNormalized = pd.DataFrame.from_records(ctMinMaxkdd.fit_transform(nslkddTrainEncoded), columns=ctMinMaxkdd.get_feature_names_out())\n",
    "unswTrainNormalized  = pd.DataFrame.from_records(ctMinMaxunsw.fit_transform(unswTrainEncoded), columns=ctMinMaxunsw.get_feature_names_out())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the label column the last column\n",
    "reorderedColumns = [col for col in unswTrainNormalized.columns if col != 'label'] + ['label']\n",
    "unswTrainNormalized = unswTrainNormalized[reorderedColumns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the same columns as Khamassi et al.\n",
    "for col in removedCols:\n",
    "    del nslkddTrainNormalized[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nslkdd10000 = nslkddTrainNormalized.sample(10000, random_state=86)\n",
    "nslkdd15000 = nslkddTrainNormalized.sample(15000, random_state=86)\n",
    "nslkdd20000 = nslkddTrainNormalized.sample(20000, random_state=86)\n",
    "\n",
    "unsw10000 = unswTrainNormalized.sample(10000, random_state=73)\n",
    "unsw15000 = unswTrainNormalized.sample(15000, random_state=73)\n",
    "unsw20000 = unswTrainNormalized.sample(20000, random_state=73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split in 10 folds\n",
    "nslkdd10000folds = []\n",
    "nslkdd15000folds = []\n",
    "nslkdd20000folds = []\n",
    "\n",
    "unsw10000folds = []\n",
    "unsw15000folds = []\n",
    "unsw20000folds = []\n",
    "\n",
    "for i in range(10):\n",
    "    nslkdd10000folds.append((nslkdd10000.iloc[0+(1000*i):1000*(i+1),:-1], nslkdd10000.iloc[0+(1000*i):1000*(i+1),-1:].labels))\n",
    "    nslkdd15000folds.append((nslkdd15000.iloc[0+(1500*i):1500*(i+1),:-1], nslkdd15000.iloc[0+(1500*i):1500*(i+1),-1:].labels))\n",
    "    nslkdd20000folds.append((nslkdd20000.iloc[0+(2000*i):2000*(i+1),:-1], nslkdd20000.iloc[0+(2000*i):2000*(i+1),-1:].labels))\n",
    "\n",
    "    unsw10000folds.append((unsw10000.iloc[0+(1000*i):1000*(i+1),:-1], unsw10000.iloc[0+(1000*i):1000*(i+1),-1:].label))\n",
    "    unsw15000folds.append((unsw15000.iloc[0+(1500*i):1500*(i+1),:-1], unsw15000.iloc[0+(1500*i):1500*(i+1),-1:].label))\n",
    "    unsw20000folds.append((unsw20000.iloc[0+(2000*i):2000*(i+1),:-1], unsw20000.iloc[0+(2000*i):2000*(i+1),-1:].label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# split in 10 folds\n",
    "nslkdd10000folds = []\n",
    "nslkdd15000folds = []\n",
    "nslkdd20000folds = []\n",
    "\n",
    "unsw10000folds = []\n",
    "unsw15000folds = []\n",
    "unsw20000folds = []\n",
    "\n",
    "for i in range(10):\n",
    "    nslkdd10000folds.append((nslkdd10000.iloc[0+(1000*i):1000*(i+1),:-1], nslkdd10000.iloc[0+(1000*i):1000*(i+1),-1:].labels))\n",
    "    nslkdd15000folds.append((nslkdd15000.iloc[0+(1500*i):1500*(i+1),:-1], nslkdd15000.iloc[0+(1500*i):1500*(i+1),-1:].labels))\n",
    "    nslkdd20000folds.append((nslkdd20000.iloc[0+(2000*i):2000*(i+1),:-1], nslkdd20000.iloc[0+(2000*i):2000*(i+1),-1:].labels))\n",
    "\n",
    "    unsw10000folds.append((unsw10000.iloc[0+(1000*i):1000*(i+1),:-1], unsw10000.iloc[0+(1000*i):1000*(i+1),-1:].label))\n",
    "    unsw15000folds.append((unsw15000.iloc[0+(1500*i):1500*(i+1),:-1], unsw15000.iloc[0+(1500*i):1500*(i+1),-1:].label))\n",
    "    unsw20000folds.append((unsw20000.iloc[0+(2000*i):2000*(i+1),:-1], unsw20000.iloc[0+(2000*i):2000*(i+1),-1:].label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify best folds for GA-LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='newton-cg', random_state=21, penalty='none', n_jobs=-1)\n",
    "\n",
    "datasetskdd = [nslkdd10000folds, nslkdd15000folds, nslkdd20000folds]\n",
    "datasetsunsw = [unsw10000folds, unsw15000folds, unsw20000folds]\n",
    "bestfoldskdd = []\n",
    "bestfoldsunsw = []\n",
    "\n",
    "for datasetwithfolds in datasetskdd:\n",
    "    bestscore = 0\n",
    "    bestpermutation = None\n",
    "    for permutation in permutations(datasetwithfolds, 2):\n",
    "        trainset = permutation[0]\n",
    "        testset = permutation[1]\n",
    "        lr.fit(trainset[0], trainset[1])\n",
    "\n",
    "        score = lr.score(testset[0], testset[1])\n",
    "        if score > bestscore:\n",
    "            bestscore = score\n",
    "            bestpermutation = permutation\n",
    "    \n",
    "    print(bestscore)\n",
    "    bestfoldskdd.append(bestpermutation)\n",
    "\n",
    "print(\"Search for best folds in UNSW-NB15\")\n",
    "for datasetwithfolds in datasetsunsw:\n",
    "    bestscore = 0\n",
    "    bestpermutation = None\n",
    "    for permutation in permutations(datasetwithfolds, 2):\n",
    "        trainset = permutation[0]\n",
    "        testset = permutation[1]\n",
    "        lr.fit(trainset[0], trainset[1])\n",
    "\n",
    "        score = lr.score(testset[0], testset[1])\n",
    "        if score > bestscore:\n",
    "            bestscore = score\n",
    "            bestpermutation = permutation\n",
    "    \n",
    "    print(bestscore)\n",
    "    bestfoldsunsw.append(bestpermutation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GA-LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(solver=\"newton-cg\", random_state=21, penalty='none', n_jobs=-1)\n",
    "geneticalg = GeneticSelection(estimator, verbose=1, n_population=30, n_generations=1000, estimator_weight=0.8, crossover_proba=0.9, mutation_proba=0.03)\n",
    "selectedFeatures = []\n",
    "i = 0\n",
    "\n",
    "print(\"Selecting features for NSL-KDD\")\n",
    "for bestfolds in bestfoldskdd:\n",
    "    start = time.time()\n",
    "    if i == 0:\n",
    "        print(\"Selecting features based on 1000 instances fold\")\n",
    "    elif i == 1:\n",
    "        print(\"Selecting features based on 1500 instances fold\")\n",
    "    elif i == 2:\n",
    "        print(\"Selecting features based on 2000 instances fold\")\n",
    "\n",
    "    trainset = bestfolds[0]\n",
    "    testset = bestfolds[1]\n",
    "    geneticalg = geneticalg.fit(X=trainset[0].values, y=trainset[1], X_test=testset[0].values, y_test=testset[1])\n",
    "    selectedFeatures.append(geneticalg.support_)\n",
    "    print(geneticalg.n_features_)\n",
    "    i += 1\n",
    "    end = time.time()\n",
    "    print(\"Execution time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Selecting features for UNSW-NB15\")\n",
    "for bestfolds in bestfoldsunsw:\n",
    "    start = time.time()\n",
    "    if i == 0:\n",
    "        print(\"Selecting features based on 1000 instances fold\")\n",
    "    elif i == 1:\n",
    "        print(\"Selecting features based on 1500 instances fold\")\n",
    "    elif i == 2:\n",
    "        print(\"Selecting features based on 2000 instances fold\")\n",
    "\n",
    "    trainset = bestfolds[0]\n",
    "    testset = bestfolds[1]\n",
    "    geneticalg = geneticalg.fit(X=trainset[0].values, y=trainset[1], X_test=testset[0].values, y_test=testset[1])\n",
    "    selectedFeatures.append(geneticalg.support_)\n",
    "    print(geneticalg.n_features_)\n",
    "    i += 1\n",
    "    end = time.time()\n",
    "    print(\"Execution time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selectedFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KBinsDiscretizer.get_feature_names_out = (lambda self, names=None: self.feature_names_in_)\n",
    "\n",
    "toDiscretizeNsl = list(nslkdd10000.select_dtypes(include=['float64', 'int64']).columns)\n",
    "toDiscretizeNsl.remove('labels')\n",
    "\n",
    "toDiscretizeUNSW = list(unsw10000.select_dtypes(include=['float64', 'int64']).columns)\n",
    "toDiscretizeUNSW.remove('label')\n",
    "\n",
    "num_discretization= Pipeline([\n",
    "    ('num_discretizer', KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='uniform'))\n",
    "])\n",
    "\n",
    "ctDiscretizationNSL = ColumnTransformer([\n",
    "    ('num', num_discretization, toDiscretizeNsl)\n",
    "], remainder=\"passthrough\", verbose_feature_names_out=False)\n",
    "\n",
    "ctDiscretizationUNSW = ColumnTransformer([\n",
    "    ('num', num_discretization, toDiscretizeUNSW)\n",
    "], remainder=\"passthrough\", verbose_feature_names_out=False)\n",
    "\n",
    "df_norm_NSL10000 = pd.DataFrame.from_records(ctDiscretizationNSL.fit_transform(nslkdd10000), columns=ctDiscretizationNSL.get_feature_names_out())\n",
    "df_norm_UNSW10000 = pd.DataFrame.from_records(ctDiscretizationUNSW.fit_transform(unsw10000), columns=ctDiscretizationUNSW.get_feature_names_out())\n",
    "df_norm_NSL15000 = pd.DataFrame.from_records(ctDiscretizationNSL.fit_transform(nslkdd15000), columns=ctDiscretizationNSL.get_feature_names_out())\n",
    "df_norm_UNSW15000 = pd.DataFrame.from_records(ctDiscretizationUNSW.fit_transform(unsw15000), columns=ctDiscretizationUNSW.get_feature_names_out())\n",
    "df_norm_NSL20000 = pd.DataFrame.from_records(ctDiscretizationNSL.fit_transform(nslkdd20000), columns=ctDiscretizationNSL.get_feature_names_out())\n",
    "df_norm_UNSW20000 = pd.DataFrame.from_records(ctDiscretizationUNSW.fit_transform(unsw20000), columns=ctDiscretizationUNSW.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsNSLKDD10000 = df_norm_NSL10000['labels']\n",
    "del df_norm_NSL10000['labels']\n",
    "labelsNSLKDD15000 = df_norm_NSL15000['labels']\n",
    "del df_norm_NSL15000['labels']\n",
    "labelsNSLKDD20000 = df_norm_NSL20000['labels']\n",
    "del df_norm_NSL20000['labels']\n",
    "\n",
    "labelsUNSW10000 = df_norm_UNSW10000['label']\n",
    "del df_norm_UNSW10000['label']\n",
    "labelsUNSW15000 = df_norm_UNSW15000['label']\n",
    "del df_norm_UNSW15000['label']\n",
    "labelsUNSW20000 = df_norm_UNSW20000['label']\n",
    "del df_norm_UNSW20000['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSLKDD10000Features = {'chi2':[], 'MI': [], 'SU': []}\n",
    "NSLKDD15000Features = {'chi2':[], 'MI': [], 'SU': []}\n",
    "NSLKDD20000Features = {'chi2':[], 'MI': [], 'SU': []}\n",
    "UNSW10000Features = {'chi2':[], 'MI': [], 'SU': []}\n",
    "UNSW15000Features = {'chi2':[], 'MI': [], 'SU': []}\n",
    "UNSW20000Features = {'chi2':[], 'MI': [], 'SU': []}\n",
    "\n",
    "for i in range(df_norm_NSL10000.shape[1]):\n",
    "\n",
    "    NSLKDD10000Features['chi2'].append(SelectKBest(chi2, k=i+1).fit(df_norm_NSL10000, labelsNSLKDD10000).get_feature_names_out(input_features=None))\n",
    "    NSLKDD10000Features['MI'].append(SelectKBest(mutual_info_classif, k=i+1).fit(df_norm_NSL10000, labelsNSLKDD10000).get_feature_names_out(input_features=None))\n",
    "    NSLKDD10000Features['SU'].append(SelectKBest(filters.univariate.su_measure, k=i+1).fit(df_norm_NSL10000, labelsNSLKDD10000).get_feature_names_out(input_features=None))\n",
    "\n",
    "    NSLKDD15000Features['chi2'].append(SelectKBest(chi2, k=i+1).fit(df_norm_NSL15000, labelsNSLKDD15000).get_feature_names_out(input_features=None))\n",
    "    NSLKDD15000Features['MI'].append(SelectKBest(mutual_info_classif, k=i+1).fit(df_norm_NSL15000, labelsNSLKDD15000).get_feature_names_out(input_features=None))\n",
    "    NSLKDD15000Features['SU'].append(SelectKBest(filters.univariate.su_measure, k=i+1).fit(df_norm_NSL15000, labelsNSLKDD15000).get_feature_names_out(input_features=None))\n",
    "\n",
    "    NSLKDD20000Features['chi2'].append(SelectKBest(chi2, k=i+1).fit(df_norm_NSL20000, labelsNSLKDD20000).get_feature_names_out(input_features=None))\n",
    "    NSLKDD20000Features['MI'].append(SelectKBest(mutual_info_classif, k=i+1).fit(df_norm_NSL20000, labelsNSLKDD20000).get_feature_names_out(input_features=None))\n",
    "    NSLKDD20000Features['SU'].append(SelectKBest(filters.univariate.su_measure, k=i+1).fit(df_norm_NSL20000, labelsNSLKDD20000).get_feature_names_out(input_features=None))\n",
    "\n",
    "for i in range(df_norm_UNSW10000.shape[1]):\n",
    "\n",
    "    UNSW10000Features['chi2'].append(SelectKBest(chi2, k=i+1).fit(df_norm_UNSW10000, labelsUNSW10000).get_feature_names_out(input_features=None))\n",
    "    UNSW10000Features['MI'].append(SelectKBest(mutual_info_classif, k=i+1).fit(df_norm_UNSW10000, labelsUNSW10000).get_feature_names_out(input_features=None))\n",
    "    UNSW10000Features['SU'].append(SelectKBest(filters.univariate.su_measure, k=i+1).fit(df_norm_UNSW10000, labelsUNSW10000).get_feature_names_out(input_features=None))\n",
    "\n",
    "    UNSW15000Features['chi2'].append(SelectKBest(chi2, k=i+1).fit(df_norm_UNSW15000, labelsUNSW15000).get_feature_names_out(input_features=None))\n",
    "    UNSW15000Features['MI'].append(SelectKBest(mutual_info_classif, k=i+1).fit(df_norm_UNSW15000, labelsUNSW15000).get_feature_names_out(input_features=None))\n",
    "    UNSW15000Features['SU'].append(SelectKBest(filters.univariate.su_measure, k=i+1).fit(df_norm_UNSW15000, labelsUNSW15000).get_feature_names_out(input_features=None))\n",
    "\n",
    "    UNSW20000Features['chi2'].append(SelectKBest(chi2, k=i+1).fit(df_norm_UNSW20000, labelsUNSW20000).get_feature_names_out(input_features=None))\n",
    "    UNSW20000Features['MI'].append(SelectKBest(mutual_info_classif, k=i+1).fit(df_norm_UNSW20000, labelsUNSW20000).get_feature_names_out(input_features=None))\n",
    "    UNSW20000Features['SU'].append(SelectKBest(filters.univariate.su_measure, k=i+1).fit(df_norm_UNSW20000, labelsUNSW20000).get_feature_names_out(input_features=None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NSLKDD10000Features.pickle', 'wb') as f:\n",
    "    pickle.dump(NSLKDD10000Features, f)\n",
    "\n",
    "with open('NSLKDD15000Features.pickle', 'wb') as f:\n",
    "    pickle.dump(NSLKDD15000Features, f)\n",
    "\n",
    "with open('NSLKDD20000Features.pickle', 'wb') as f:\n",
    "    pickle.dump(NSLKDD20000Features, f)\n",
    "\n",
    "with open('UNSWNB15_10000Features.pickle', 'wb') as f:\n",
    "    pickle.dump(UNSW10000Features,f)\n",
    "\n",
    "with open('UNSWNB15_15000Features.pickle', 'wb') as f:\n",
    "    pickle.dump(UNSW15000Features, f)\n",
    "\n",
    "with open('UNSWNB15_20000Features.pickle', 'wb') as f:\n",
    "    pickle.dump(UNSW20000Features, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

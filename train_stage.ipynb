{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, matthews_corrcoef, confusion_matrix, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the columns for NSL-KDD\n",
    "\n",
    "columns = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "                    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "                    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "                    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "                    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "                    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "                    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "                    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "                    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "                    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"labels\"]\n",
    "\n",
    "\n",
    "nslkddTrain = pd.read_csv('./NSL-KDD/KDDTrain+.txt', sep=\",\", header=None, usecols = [i for i in range(42)], names=columns)\n",
    "\n",
    "# Convert boolean features to objects to avoid misinterpretation in NSL-KDD\n",
    "nslkddTrain['land'] = nslkddTrain['land'].astype('object', copy=False)\n",
    "nslkddTrain['logged_in'] = nslkddTrain['logged_in'].astype('object', copy=False)\n",
    "nslkddTrain['urgent'] = nslkddTrain['urgent'].astype('object', copy=False)\n",
    "nslkddTrain['is_host_login'] = nslkddTrain['is_host_login'].astype('object', copy=False)\n",
    "nslkddTrain['is_guest_login'] = nslkddTrain['is_guest_login'].astype('object', copy=False)\n",
    "\n",
    "# Import UNSW-NB15 train set\n",
    "\n",
    "unswTrain = pd.DataFrame()\n",
    "unswTrain = pd.read_csv('./UNSW-NB15/UNSW_NB15_training-set.csv', sep=\",\", na_values=[' '])\n",
    "\n",
    "# Convert boolean features to objects to avoid misinterpretation in UNSW-NB15\n",
    "unswTrain['is_ftp_login'] = unswTrain['is_ftp_login'].astype('object', copy=False)\n",
    "unswTrain['is_sm_ips_ports'] = unswTrain['is_sm_ips_ports'].astype('object', copy=False)\n",
    "unswTrain['label'] = unswTrain['label'].astype('object', copy=False)\n",
    "del unswTrain[\"label\"]\n",
    "del unswTrain[\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make class categories in NSL-KDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustersMapping = {\n",
    "    'normal': 'normal',\n",
    "\n",
    "    'back': 'DoS',\n",
    "    'land': 'DoS',\n",
    "    'neptune': 'DoS',\n",
    "    'pod': 'DoS',\n",
    "    'smurf': 'DoS',\n",
    "    'teardrop': 'DoS',\n",
    "    'mailbomb': 'DoS',\n",
    "    'apache2': 'DoS',\n",
    "    'processtable': 'DoS',\n",
    "    'udpstorm': 'DoS',\n",
    "    \n",
    "\n",
    "    'ipsweep': 'Probe',\n",
    "    'nmap': 'Probe',\n",
    "    'portsweep': 'Probe',\n",
    "    'satan': 'Probe',\n",
    "    'mscan': 'Probe',\n",
    "    'saint': 'Probe',\n",
    "\n",
    "    'ftp_write': 'R2L',\n",
    "    'guess_passwd': 'R2L',\n",
    "    'imap': 'R2L',\n",
    "    'multihop': 'R2L',\n",
    "    'phf': 'R2L',\n",
    "    'spy': 'R2L',\n",
    "    'warezclient': 'R2L',\n",
    "    'warezmaster': 'R2L',\n",
    "    'sendmail': 'R2L',\n",
    "    'named': 'R2L',\n",
    "    'snmpgetattack': 'R2L',\n",
    "    'snmpguess': 'R2L',\n",
    "    'xlock': 'R2L',\n",
    "    'xsnoop': 'R2L',\n",
    "    'sqlattack': 'R2L',\n",
    "    \n",
    "    \n",
    "\n",
    "    'buffer_overflow': 'U2R',\n",
    "    'loadmodule': 'U2R',\n",
    "    'perl': 'U2R',\n",
    "    'rootkit': 'U2R',\n",
    "    'ps': 'U2R',\n",
    "    'xterm': 'U2R',\n",
    "    'httptunnel': 'U2R',\n",
    "    'worm': 'U2R',\n",
    "    \n",
    "}\n",
    "\n",
    "nslkddTrain['labels'] = nslkddTrain['labels'].map(clustersMapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling of NSLK-KDD while keeping the same distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nslkddTrain['labels'].value_counts())\n",
    "\n",
    "# Get the distribution of each label\n",
    "for value in nslkddTrain['labels'].value_counts():\n",
    "    print(value/nslkddTrain.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalCount = round(0.5345828074269883 * 50000)\n",
    "DoSCount = round(0.3645781238836894 * 50000)\n",
    "ProbeCount = round(0.09252776388591206 * 50000)\n",
    "R2LCount = round(0.007898517936383194 * 50000)\n",
    "U2RCount = round(0.00041278686702706137 * 50000)\n",
    "\n",
    "grouped = nslkddTrain.groupby(\"labels\")\n",
    "for element in grouped:\n",
    "    if element[0] == 'DoS':\n",
    "        dosdataset = element[1].sample(DoSCount)\n",
    "    elif element[0] == 'normal':\n",
    "        normaldataset = element[1].sample(normalCount)\n",
    "    elif element[0] == 'Probe':\n",
    "        probedataset = element[1].sample(ProbeCount)\n",
    "    elif element[0] == 'R2L':\n",
    "        r2ldataset = element[1].sample(R2LCount)\n",
    "    elif element[0] == 'U2R':\n",
    "        u2rdataset = element[1].sample(U2RCount)\n",
    "\n",
    "# Concat all datasets\n",
    "nslkddTrainSampled = pd.concat([normaldataset, dosdataset, probedataset, r2ldataset, u2rdataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling of UNSW-NB15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unswTrain['attack_cat'].value_counts())\n",
    "\n",
    "# Get the distribution of each label\n",
    "for value in unswTrain['attack_cat'].value_counts():\n",
    "    print(value/unswTrain.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormalCount = round(0.44939999028324346 * 50000)\n",
    "GenericCount = round(0.2292061409901375 * 50000)\n",
    "ExploitsCount = round(0.13520866734683962 * 50000)\n",
    "FuzzersCount = round(0.07362872273235194 * 50000)\n",
    "DoSCount = round(0.049664771899140064 * 50000)\n",
    "ReconnaissanceCount = round(0.04246222610892484 * 50000)\n",
    "AnalysisCount = round(0.008222805227615022 * 50000)\n",
    "BackdoorCount = round(0.007081086333381917 * 50000)\n",
    "ShellcodeCount = round(0.004591167468299082 * 50000)\n",
    "WormsCount = round(0.0005344216100665598 * 50000)\n",
    "\n",
    "grouped = unswTrain.groupby(\"attack_cat\")\n",
    "for element in grouped:\n",
    "    if element[0] == 'Normal':\n",
    "        normaldataset = element[1].sample(NormalCount)\n",
    "    elif element[0] == 'Generic':\n",
    "        genericdataset = element[1].sample(GenericCount)\n",
    "    elif element[0] == 'Exploits':\n",
    "        exploitsdataset = element[1].sample(ExploitsCount)\n",
    "    elif element[0] == 'Fuzzers':\n",
    "        fuzzersdataset = element[1].sample(FuzzersCount)\n",
    "    elif element[0] == 'DoS':\n",
    "        dosdataset = element[1].sample(DoSCount)\n",
    "    elif element[0] == 'Reconnaissance':\n",
    "        reconnaissancedataset = element[1].sample(ReconnaissanceCount)\n",
    "    elif element[0] == 'Analysis':\n",
    "        analysisdataset = element[1].sample(AnalysisCount)\n",
    "    elif element[0] == 'Backdoor':\n",
    "        backdoordataset = element[1].sample(BackdoorCount)\n",
    "    elif element[0] == 'Shellcode':\n",
    "        shellcodedataset = element[1].sample(ShellcodeCount)\n",
    "    elif element[0] == 'Worms':\n",
    "        wormsdataset = element[1].sample(WormsCount +1)\n",
    "\n",
    "# Concat all datasets\n",
    "unswTrainSampled = pd.concat([normaldataset, genericdataset, exploitsdataset, fuzzersdataset, dosdataset, reconnaissancedataset, analysisdataset, backdoordataset, shellcodedataset, wormsdataset])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nslkddTrainLabels = nslkddTrainSampled.labels\n",
    "lekdd = LabelEncoder()\n",
    "lekdd.fit(nslkddTrainLabels)\n",
    "nslkddTrainLabels = lekdd.transform(nslkddTrainLabels)\n",
    "del nslkddTrainSampled['labels']\n",
    "\n",
    "unswTrainLabels = unswTrainSampled.attack_cat\n",
    "leunsw = LabelEncoder()\n",
    "leunsw.fit(unswTrainLabels)\n",
    "unswTrainLabels = leunsw.transform(unswTrainLabels)\n",
    "del unswTrainSampled['attack_cat']\n",
    "\n",
    "# Perform ordinal encoding on remaining string values\n",
    "toEncodeKdd = list(nslkddTrainSampled.select_dtypes(include=['object']).columns)\n",
    "toEncodeUnsw = list(unswTrainSampled.select_dtypes(include=['object']).columns)\n",
    "\n",
    "OrdinalEncoder.get_feature_names_out = (lambda self, names=None: self.feature_names_in_)\n",
    "\n",
    "cat_encoding = Pipeline([\n",
    "    ('cat_encoder', OrdinalEncoder())\n",
    "    ])\n",
    "\n",
    "ctEncodingKdd = ColumnTransformer([\n",
    "    ('cat', cat_encoding, toEncodeKdd)\n",
    "    ], remainder=\"passthrough\", verbose_feature_names_out=False)\n",
    "\n",
    "ctEncodingUnsw = ColumnTransformer([\n",
    "('cat', cat_encoding, toEncodeUnsw)\n",
    "], remainder=\"passthrough\", verbose_feature_names_out=False)\n",
    "\n",
    "nslkddTrainEncoded = pd.DataFrame.from_records(ctEncodingKdd.fit_transform(nslkddTrainSampled), columns=ctEncodingKdd.get_feature_names_out())\n",
    "unswTrainEncoded = pd.DataFrame.from_records(ctEncodingUnsw.fit_transform(unswTrainSampled), columns=ctEncodingUnsw.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log base 10 to columns containing large values\n",
    "largevalueskdd = ['duration', 'src_bytes', 'dst_bytes', 'num_compromised', 'num_root', 'count', 'srv_count', 'dst_host_count', 'dst_host_srv_count']\n",
    "largevaluesunsw = ['spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'sload', 'dload', 'sloss', 'dloss', 'sinpkt', 'dinpkt', 'sjit', 'djit', 'stcpb', 'dtcpb', 'smean', 'dmean']\n",
    "\n",
    "for colname in largevalueskdd:\n",
    "    nslkddTrainEncoded[colname] = np.log10(nslkddTrainEncoded[colname])\n",
    "    nslkddTrainEncoded.replace([-np.inf], 0, inplace=True)\n",
    "\n",
    "for colname in largevaluesunsw:\n",
    "    unswTrainEncoded[colname] = np.log10(unswTrain[colname])\n",
    "    unswTrainEncoded[colname].replace([-np.inf], 0, inplace=True)\n",
    "\n",
    "# Apply minmax scaler on numerical values\n",
    "MinMaxScaler.get_feature_names_out = (lambda self, names=None: self.feature_names_in_)\n",
    "\n",
    "numColskdd = list(nslkddTrainEncoded.select_dtypes(include=['float64', 'int64']).columns)\n",
    "numColsunsw = list(unswTrainEncoded.select_dtypes(include=['float64', 'int64']).columns)\n",
    "\n",
    "\n",
    "num_minmaxscaling = Pipeline([\n",
    "    ('num_minmaxscaling', MinMaxScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "ctMinMaxkdd = ColumnTransformer([\n",
    "    ('minmax', num_minmaxscaling, numColskdd)\n",
    "], remainder=\"passthrough\", verbose_feature_names_out=False)\n",
    "\n",
    "ctMinMaxunsw = ColumnTransformer([\n",
    "    ('minmax', num_minmaxscaling, numColsunsw)\n",
    "], remainder=\"passthrough\", verbose_feature_names_out=False)\n",
    "\n",
    "\n",
    "nslkddTrainNormalized = pd.DataFrame.from_records(ctMinMaxkdd.fit_transform(nslkddTrainEncoded), columns=ctMinMaxkdd.get_feature_names_out())\n",
    "unswTrainNormalized  = pd.DataFrame.from_records(ctMinMaxunsw.fit_transform(unswTrainEncoded), columns=ctMinMaxunsw.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## Classification on full train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_rate(y, y_predicted):\n",
    "    matrix = confusion_matrix(y_true=y, y_pred=y_predicted)\n",
    "    tp = np.diag(matrix)\n",
    "    fn = matrix.sum(axis=1) - np.diag(matrix)\n",
    "    fn = fn.astype(float)\n",
    "    tp = tp.astype(float)\n",
    "    dr = tp/(tp+fn)\n",
    "    print(dr)\n",
    "    return dr.mean()\n",
    "\n",
    "\n",
    "def false_alarm_rate(y, y_predicted):\n",
    "    matrix = confusion_matrix(y_true=y, y_pred=y_predicted)\n",
    "    fp = matrix.sum(axis=0) - np.diag(matrix) \n",
    "    fn = matrix.sum(axis=1) - np.diag(matrix)\n",
    "    tp = np.diag(matrix)\n",
    "    tn = matrix.sum() - (fp + fn + tp)\n",
    "\n",
    "    fp = fp.astype(float)\n",
    "    fn = fn.astype(float)\n",
    "    tp = tp.astype(float)\n",
    "    tn = tn.astype(float)\n",
    "    far = fp/(tn+fp)\n",
    "    return far.mean()\n",
    "\n",
    "\n",
    "def F1Score(y_true,y_pred):\n",
    "    return f1_score(y_true, y_pred, average=\"micro\")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=64)\n",
    "svm = LinearSVC(dual=False, random_state=34)\n",
    "lr = LogisticRegression(solver='newton-cg', random_state=21, penalty='none', n_jobs=-1)\n",
    "start = time.time()\n",
    "crossValidationRf = cross_validate(rf, nslkddTrainNormalized, nslkddTrainLabels, scoring={'accuracy': make_scorer(accuracy_score, greater_is_better=True), 'matthews': make_scorer(matthews_corrcoef, greater_is_better=True), 'f1':make_scorer(F1Score, greater_is_better=True), 'detection_rate':make_scorer(detection_rate, greater_is_better=True), 'false_alarm_rate':make_scorer(false_alarm_rate, greater_is_better=False)}, cv=10)\n",
    "end = time.time()\n",
    "print('Time taken for Cross validation RF: ', end-start)\n",
    "start = time.time()\n",
    "crossValidationSvm = cross_validate(svm, nslkddTrainNormalized, nslkddTrainLabels, scoring={'accuracy': make_scorer(accuracy_score, greater_is_better=True), 'matthews': make_scorer(matthews_corrcoef, greater_is_better=True), 'f1':make_scorer(F1Score, greater_is_better=True), 'detection_rate':make_scorer(detection_rate, greater_is_better=True), 'false_alarm_rate':make_scorer(false_alarm_rate, greater_is_better=False)}, cv=10)\n",
    "end = time.time()\n",
    "print('Time taken for Cross validation SVM: ', end-start)\n",
    "start = time.time()\n",
    "crossValidationLr = cross_validate(lr, nslkddTrainNormalized, nslkddTrainLabels, scoring={'accuracy': make_scorer(accuracy_score, greater_is_better=True), 'matthews': make_scorer(matthews_corrcoef, greater_is_better=True), 'f1':make_scorer(F1Score, greater_is_better=True), 'detection_rate':make_scorer(detection_rate, greater_is_better=True), 'false_alarm_rate':make_scorer(false_alarm_rate, greater_is_better=False)}, cv=10, n_jobs=-1)\n",
    "end = time.time()\n",
    "print('Time taken for Cross validation LR: ', end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = crossValidationRf['test_accuracy'].mean()\n",
    "matthews = crossValidationRf['test_matthews'].mean()\n",
    "f1 = crossValidationRf['test_f1'].mean()\n",
    "far = crossValidationRf['test_false_alarm_rate'].mean()\n",
    "dr = crossValidationRf['test_detection_rate'].mean()\n",
    "print(accuracies, matthews, f1, far, dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = crossValidationSvm['test_accuracy'].mean()\n",
    "matthews = crossValidationSvm['test_matthews'].mean()\n",
    "f1 = crossValidationSvm['test_f1'].mean()\n",
    "far = crossValidationSvm['test_false_alarm_rate'].mean()\n",
    "dr = crossValidationSvm['test_detection_rate'].mean()\n",
    "print(accuracies, matthews, f1, far, dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = crossValidationLr['test_accuracy'].mean()\n",
    "matthews = crossValidationLr['test_matthews'].mean()\n",
    "f1 = crossValidationLr['test_f1'].mean()\n",
    "far = crossValidationLr['test_false_alarm_rate'].mean()\n",
    "dr = crossValidationLr['test_detection_rate'].mean()\n",
    "print(accuracies, matthews, f1, far, dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=64)\n",
    "svm = LinearSVC(dual=False, random_state=34)\n",
    "lr = LogisticRegression(solver='newton-cg', random_state=21, penalty='none', n_jobs=-1)\n",
    "start = time.time()\n",
    "crossValidationRf = cross_validate(rf, unswTrainNormalized, unswTrainLabels, scoring={'accuracy': make_scorer(accuracy_score, greater_is_better=True), 'matthews': make_scorer(matthews_corrcoef, greater_is_better=True), 'f1':make_scorer(F1Score, greater_is_better=True), 'detection_rate':make_scorer(detection_rate, greater_is_better=True), 'false_alarm_rate':make_scorer(false_alarm_rate, greater_is_better=False)}, cv=10)\n",
    "end = time.time()\n",
    "print('Time taken for Cross validation RF: ', end-start)\n",
    "start = time.time()\n",
    "crossValidationSvm = cross_validate(svm, unswTrainNormalized, unswTrainLabels, scoring={'accuracy': make_scorer(accuracy_score, greater_is_better=True), 'matthews': make_scorer(matthews_corrcoef, greater_is_better=True), 'f1':make_scorer(F1Score, greater_is_better=True), 'detection_rate':make_scorer(detection_rate, greater_is_better=True), 'false_alarm_rate':make_scorer(false_alarm_rate, greater_is_better=False)}, cv=10)\n",
    "end = time.time()\n",
    "print('Time taken for Cross validation SVM: ', end-start)\n",
    "start = time.time()\n",
    "crossValidationLr = cross_validate(lr, unswTrainNormalized, unswTrainLabels, scoring={'accuracy': make_scorer(accuracy_score, greater_is_better=True), 'matthews': make_scorer(matthews_corrcoef, greater_is_better=True), 'f1':make_scorer(F1Score, greater_is_better=True), 'detection_rate':make_scorer(detection_rate, greater_is_better=True), 'false_alarm_rate':make_scorer(false_alarm_rate, greater_is_better=False)}, cv=10, n_jobs=-1)\n",
    "end = time.time()\n",
    "print('Time taken for Cross validation LR: ', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = crossValidationRf['test_accuracy'].mean()\n",
    "matthews = crossValidationRf['test_matthews'].mean()\n",
    "f1 = crossValidationRf['test_f1'].mean()\n",
    "far = crossValidationRf['test_false_alarm_rate'].mean()\n",
    "dr = crossValidationRf['test_detection_rate'].mean()\n",
    "print(accuracies, matthews, f1, far, dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = crossValidationSvm['test_accuracy'].mean()\n",
    "matthews = crossValidationSvm['test_matthews'].mean()\n",
    "f1 = crossValidationSvm['test_f1'].mean()\n",
    "far = crossValidationSvm['test_false_alarm_rate'].mean()\n",
    "dr = crossValidationSvm['test_detection_rate'].mean()\n",
    "print(accuracies, matthews, f1, far, dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = crossValidationLr['test_accuracy'].mean()\n",
    "matthews = crossValidationLr['test_matthews'].mean()\n",
    "f1 = crossValidationLr['test_f1'].mean()\n",
    "far = crossValidationLr['test_false_alarm_rate'].mean()\n",
    "dr = crossValidationLr['test_detection_rate'].mean()\n",
    "print(accuracies, matthews, f1, far, dr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with reduced feature sets from GA-LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = ['service', 'is_guest_login', 'src_bytes', 'dst_bytes', 'hot', 'num_compromised', 'num_root', 'num_file_creations', 'num_access_files', 'count', 'srv_count', 'serror_rate', 'rerror_rate', 'diff_srv_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate']\n",
    "A2 = [\"service\", \"is_guest_login\", \"src_bytes\", \"dst_bytes\", \"hot\", \"num_file_creations\", \"count\", \"dst_host_srv_count\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \"dst_host_srv_diff_host_rate\", \"dst_host_srv_serror_rate\"]\n",
    "A3 = ['service', 'is_guest_login', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'hot', 'num_file_creations', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_srv_count', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate']\n",
    "\n",
    "B1 = ['protocol_type', 'flag', 'is_guest_login', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'hot', 'num_access_files', 'count', 'srv_count', 'serror_rate', 'rerror_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_srv_rerror_rate']\n",
    "B2 = ['flag', 'is_guest_login', 'dst_bytes', 'hot', 'count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_srv_serror_rate', 'dst_host_srv_rerror_rate']\n",
    "B3 = ['protocol_type', 'service', 'flag', 'logged_in', 'is_guest_login', 'duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'hot', 'num_compromised', 'num_access_files', 'count', 'srv_count', 'rerror_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_srv_serror_rate']\n",
    "\n",
    "C1 = ['protocol_type', 'service', 'logged_in', 'is_guest_login', 'src_bytes', 'wrong_fragment', 'hot', 'num_compromised', 'num_access_files', 'count', 'serror_rate', 'srv_serror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_srv_serror_rate', 'dst_host_srv_rerror_rate']\n",
    "C2 = ['protocol_type', 'flag', 'is_guest_login', 'src_bytes', 'wrong_fragment', 'hot', 'num_file_creations', 'num_access_files', 'count', 'srv_count', 'serror_rate', 'rerror_rate', 'same_srv_rate', 'dst_host_same_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate']\n",
    "C3 = ['protocol_type', 'is_guest_login', 'src_bytes', 'wrong_fragment', 'hot', 'count', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate']\n",
    "\n",
    "D1 = [\"proto\", \"service\", \"state\", \"is_sm_ips_ports\", \"dur\", \"dbytes\", \"dttl\", \"sload\", \"dload\", \"dloss\", \"sinpkt\", \"swin\", \"tcprtt\", \"ackdat\",\"dmean\", \"response_body_len\", \"ct_srv_src\", \"ct_state_ttl\", \"ct_dst_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"ct_flw_http_mthd\", \"ct_src_ltm\", \"ct_srv_dst\"]\n",
    "D2 = [\"proto\", \"service\", \"state\", \"is_sm_ips_ports\", \"rate\", \"dttl\", \"sload\", \"dloss\", \"sinpkt\", \"dinpkt\", \"stcpb\", \"dwin\", \"synack\", \"dmean\", \"response_body_len\", \"ct_srv_src\", \"ct_state_ttl\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"ct_flw_http_mthd\", \"ct_srv_dst\"]\n",
    "D3 = []\n",
    "\n",
    "E1 = [\"service\", \"state\", \"spkts\", \"dttl\", \"dload\", \"sloss\", \"sinpkt\", \"dinpkt\", \"swin\", \"tcprtt\", \"smean\", \"response_body_len\", \"ct_state_ttl\", \"ct_dst_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"ct_flw_http_mthd\", \"ct_srv_dst\"]\n",
    "E2 = []\n",
    "E3 = [\"service\", \"state\", \"is_sm_ips_ports\", \"sbytes\", \"dttl\", \"sload\", \"sjit\", \"swin\", \"synack\", \"ackdat\", \"dmean\", \"response_body_len\", \"ct_srv_src\", \"ct_state_ttl\", \"ct_dst_ltm\", \"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"ct_flw_http_mthd\", \"ct_src_ltm\", \"ct_srv_dst\"]\n",
    "\n",
    "F1 = [\"proto\", \"service\", \"state\", \"spkts\", \"dpkts\", \"dbytes\", \"rate\", \"dttl\", \"sload\", \"swin\", \"dtcpb\",\"synack\", \"smean\", \"dmean\", \"response_body_len\", \"ct_srv_src\", \"ct_state_ttl\", \"ct_dst_ltm\",\"ct_dst_sport_ltm\", \"ct_dst_src_ltm\", \"ct_ftp_cmd\", \"ct_flw_http_mthd\", \"ct_srv_dst\"]\n",
    "F2 = [\"service\", \"state\", \"is_sm_ips_ports\", \"spkts\", \"dttl\", \"sloss\", \"dloss\", \"swin\", \"dwin\", \"synack\", \"dmean\",\"response_body_len\", \"ct_srv_src\", \"ct_state_ttl\", \"ct_dst_ltm\", \"ct_dst_sport_ltm\",\"ct_dst_src_ltm\", \"ct_flw_http_mthd\", \"ct_srv_dst\"]\n",
    "F3 = [\"proto\", \"service\", \"state\", \"dur\", \"dpkts\", \"dbytes\", \"sttl\", \"dttl\", \"dloss\", \"djit\", \"swin\", \"dwin\", \"tcprtt\", \"response_body_len\", \"ct_state_ttl\", \"ct_src_dport_ltm\", \"ct_dst_sport_ltm\",\"ct_dst_src_ltm\", \"ct_flw_http_mthd\", \"ct_srv_dst\"]\n",
    "\n",
    "reducedDatasetNslA1 = nslkddTrainNormalized.loc[:, A1]\n",
    "reducedDatasetNslA2 = nslkddTrainNormalized.loc[:, A2]\n",
    "reducedDatasetNslA3 = nslkddTrainNormalized.loc[:, A3]\n",
    "\n",
    "reducedDatasetNslB3 = nslkddTrainNormalized.loc[:, B3]\n",
    "\n",
    "reducedDatasetNslC3 = nslkddTrainNormalized.loc[:, C3]\n",
    "\n",
    "reducedDatasetUnswD1 = unswTrainNormalized.loc[:, D1]\n",
    "reducedDatasetUnswD2 = unswTrainNormalized.loc[:, D2]\n",
    "\n",
    "\n",
    "reducedDatasetUnswE1 = unswTrainNormalized.loc[:, E1]\n",
    "reducedDatasetUnswE3 = unswTrainNormalized.loc[:, E3]\n",
    "\n",
    "reducedDatasetUnswF1 = unswTrainNormalized.loc[:, F1]\n",
    "reducedDatasetUnswF2 = unswTrainNormalized.loc[:, F2]\n",
    "reducedDatasetUnswF3 = unswTrainNormalized.loc[:, F3]\n",
    "\n",
    "# Cross validations must have their variables changed for each dataset and cell must be run again\n",
    "start = time.time()\n",
    "crossValidationRf = cross_validate(rf, reducedDatasetNslA3, nslkddTrainLabels, scoring={'accuracy': make_scorer(accuracy_score, greater_is_better=True), 'matthews': make_scorer(matthews_corrcoef, greater_is_better=True), 'f1':make_scorer(F1Score, greater_is_better=True), 'detection_rate':make_scorer(detection_rate, greater_is_better=True), 'false_alarm_rate':make_scorer(false_alarm_rate, greater_is_better=False)}, cv=10)\n",
    "end = time.time()\n",
    "print(\"Time taken for RF: \", end-start)\n",
    "\n",
    "start = time.time()\n",
    "crossValidationSvm = cross_validate(svm, reducedDatasetNslA3, nslkddTrainLabels, scoring={'accuracy': make_scorer(accuracy_score, greater_is_better=True), 'matthews': make_scorer(matthews_corrcoef, greater_is_better=True), 'f1':make_scorer(F1Score, greater_is_better=True), 'detection_rate':make_scorer(detection_rate, greater_is_better=True), 'false_alarm_rate':make_scorer(false_alarm_rate, greater_is_better=False)}, cv=10)\n",
    "end = time.time()\n",
    "print(\"Time taken for SVM: \", end-start)\n",
    "\n",
    "start = time.time()\n",
    "crossValidationLr = cross_validate(lr, reducedDatasetNslA3, nslkddTrainLabels, scoring={'accuracy': make_scorer(accuracy_score, greater_is_better=True), 'matthews': make_scorer(matthews_corrcoef, greater_is_better=True), 'f1':make_scorer(F1Score, greater_is_better=True), 'detection_rate':make_scorer(detection_rate, greater_is_better=True), 'false_alarm_rate':make_scorer(false_alarm_rate, greater_is_better=False)}, cv=10)\n",
    "end = time.time()\n",
    "print(\"Time taken for LR: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = crossValidationRf['test_accuracy'].mean()\n",
    "matthews = crossValidationRf['test_matthews'].mean()\n",
    "f1 = crossValidationRf['test_f1'].mean()\n",
    "far = crossValidationRf['test_false_alarm_rate'].mean()\n",
    "dr = crossValidationRf['test_detection_rate'].mean()\n",
    "print(accuracies, matthews, f1, far, dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = crossValidationSvm['test_accuracy'].mean()\n",
    "matthews = crossValidationSvm['test_matthews'].mean()\n",
    "f1 = crossValidationSvm['test_f1'].mean()\n",
    "far = crossValidationSvm['test_false_alarm_rate'].mean()\n",
    "dr = crossValidationSvm['test_detection_rate'].mean()\n",
    "print(accuracies, matthews, f1, far, dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = crossValidationLr['test_accuracy'].mean()\n",
    "matthews = crossValidationLr['test_matthews'].mean()\n",
    "f1 = crossValidationLr['test_f1'].mean()\n",
    "far = crossValidationLr['test_false_alarm_rate'].mean()\n",
    "dr = crossValidationLr['test_detection_rate'].mean()\n",
    "print(accuracies, matthews, f1, far, dr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import scores from Filter-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSLKDD10000Features = None\n",
    "NSLKDD15000Features = None\n",
    "NSLKDD20000Features = None\n",
    "UNSWNB15_10000Features = None\n",
    "UNSWNB15_15000Features = None\n",
    "UNSWNB15_20000Features = None\n",
    "\n",
    "with open('NSLKDD10000Features.pickle', 'rb') as f:\n",
    "    NSLKDD10000Features = pickle.load(f)\n",
    "\n",
    "with open('NSLKDD15000Features.pickle', 'rb') as f:\n",
    "    NSLKDD15000Features = pickle.load(f)\n",
    "\n",
    "with open('NSLKDD20000Features.pickle', 'rb') as f:\n",
    "    NSLKDD20000Features = pickle.load(f)\n",
    "\n",
    "with open('UNSWNB15_10000Features.pickle', 'rb') as f:\n",
    "    UNSWNB15_10000Features = pickle.load(f)\n",
    "\n",
    "with open('UNSWNB15_15000Features.pickle', 'rb') as f:\n",
    "    UNSWNB15_15000Features = pickle.load(f)\n",
    "\n",
    "with open('UNSWNB15_20000Features.pickle', 'rb') as f:\n",
    "    UNSWNB15_20000Features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureSetsKDD = [NSLKDD10000Features, NSLKDD15000Features, NSLKDD20000Features]\n",
    "featureSetsUNSW = [UNSWNB15_10000Features, UNSWNB15_15000Features, UNSWNB15_20000Features]\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=64)\n",
    "svm = LinearSVC(dual=False, random_state=34)\n",
    "lr = LogisticRegression(solver='newton-cg', random_state=21, penalty='none')\n",
    "accuracies10000 = []\n",
    "accuracies15000 = []\n",
    "accuracies20000 = []\n",
    "\n",
    "for i in range(len(featureSetsKDD)):\n",
    "\n",
    "    featureset = featureSetsKDD[i]\n",
    "    if i == 0:\n",
    "        print('Finding right amount of features for NSL-KDD 10000')\n",
    "    elif i == 1:\n",
    "        print('Finding right amount of features for NSL-KDD 15000')\n",
    "    elif i == 2:\n",
    "        print('Finding right amount of features for NSL-KDD 20000')\n",
    "\n",
    "    start = time.time()\n",
    "    for j in range(1, len(featureset['chi2'])):\n",
    "\n",
    "        print('Nb of features: ', j+1)\n",
    "        reducedSet = nslkddTrainNormalized.loc[:, featureset['MI'][j]] # Feature set must be changed to MI, SU, or chi2 depending on what we want to compute\n",
    "        crossValidationRf = cross_validate(rf, reducedSet, nslkddTrainLabels, scoring={'accuracy': make_scorer(accuracy_score, greater_is_better=True), 'matthews': make_scorer(matthews_corrcoef, greater_is_better=True), 'f1':make_scorer(F1Score, greater_is_better=True), 'detection_rate':make_scorer(detection_rate, greater_is_better=True), 'false_alarm_rate':make_scorer(false_alarm_rate, greater_is_better=False)}, cv=10)\n",
    "\n",
    "        if i == 0:\n",
    "            accuracies10000.append(crossValidationRf['test_accuracy'].mean())\n",
    "    \n",
    "        elif i == 1:\n",
    "            accuracies15000.append(crossValidationRf['test_accuracy'].mean())\n",
    "        elif i == 2:\n",
    "            accuracies20000.append(crossValidationRf['test_accuracy'].mean())\n",
    "        \n",
    "    end = time.time()\n",
    "    print('Time to compute: ', end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the accuracies obtained at the previous cells, the most interesting must be picked as done at the next line to get its complete metrics\n",
    "reducedSet = nslkddTrainNormalized.loc[:, NSLKDD20000Features['MI'][23]]\n",
    "crossValidationRf = cross_validate(rf, reducedSet, nslkddTrainLabels, scoring={'accuracy': make_scorer(accuracy_score, greater_is_better=True), 'matthews': make_scorer(matthews_corrcoef, greater_is_better=True), 'f1':make_scorer(F1Score, greater_is_better=True), 'detection_rate':make_scorer(detection_rate, greater_is_better=True), 'false_alarm_rate':make_scorer(false_alarm_rate, greater_is_better=False)}, cv=10)\n",
    "print(crossValidationRf['test_matthews'].mean())\n",
    "f1 = crossValidationRf['test_f1'].mean()\n",
    "far = crossValidationRf['test_false_alarm_rate'].mean()\n",
    "dr = crossValidationRf['test_detection_rate'].mean()\n",
    "print(f1,far,dr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
